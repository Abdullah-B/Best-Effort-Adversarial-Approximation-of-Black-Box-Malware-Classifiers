{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "\n",
      "This is nonfunctional demonstration code that is provided for convenience. It shows\n",
      "- The MalConv structure used in our paper\n",
      "- Training procedure used in the paper\n",
      "- How to load the weights for the MalConv model that we used.\n",
      "\n",
      "It may be made functional by modifying the code to retrieve file contents by sha256\n",
      "from a user-defined URL.\n",
      "\n",
      "You may use the provided weights under the Ember AGPL-3.0 license included in the parent directory.\n",
      "We also ask that you cite the original MalConv paper and refer to the Ember paper as the implementation.\n",
      "\n",
      "(1) E. Raff, J. Barker, J. Sylvester, R. Brandon, B. Catanzaro, C. Nicholas, \"Malware Detection by Eating a Whole EXE\", in ArXiv e-prints. Oct. 2017.\n",
      "\n",
      "@ARTICLE{raff2017malware,\n",
      "  title={Malware detection by eating a whole exe},\n",
      "  author={Raff, Edward and Barker, Jon and Sylvester, Jared and Brandon, Robert and Catanzaro, Bryan and Nicholas, Charles},\n",
      "  journal={arXiv preprint arXiv:1710.09435},\n",
      "  year={2017}\n",
      "}\n",
      "\n",
      "(2) H. Anderson and P. Roth, \"EMBER: An Open Dataset for Training Static PE Malware Machine Learning Models”, in ArXiv e-prints. Apr. 2018.\n",
      "\n",
      "@ARTICLE{2018arXiv180404637A,\n",
      "  author = {{Anderson}, H.~S. and {Roth}, P.},\n",
      "  title = \"{EMBER: An Open Dataset for Training Static PE Malware Machine Learning Models}\",\n",
      "  journal = {ArXiv e-prints},\n",
      "  archivePrefix = \"arXiv\",\n",
      "  eprint = {1804.04637},\n",
      "  primaryClass = \"cs.CR\",\n",
      "  keywords = {Computer Science - Cryptography and Security},\n",
      "  year = 2018,\n",
      "  month = apr,\n",
      "  adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180404637A},\n",
      "}\n",
      "\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "'''defines the MalConv architecture.\n",
    "Adapted from https://arxiv.org/pdf/1710.09435.pdf\n",
    "Things different about our implementation and that of the original paper:\n",
    " * The paper uses batch_size = 256 and SGD(lr=0.01, momentum=0.9, decay=UNDISCLOSED, nesterov=True )\n",
    " * The paper didn't have a special EOF symbol\n",
    " * The paper allowed for up to 2MB malware sizes, we use 1.0MB because of memory on a Titan X\n",
    " '''\n",
    "\n",
    "def main(): \n",
    "    from keras.layers import Dense, Conv1D, Activation, GlobalMaxPooling1D, Input, Embedding, Multiply\n",
    "    from keras.models import Model\n",
    "    from keras import backend as K\n",
    "    from keras import metrics\n",
    "    import multi_gpu\n",
    "    import os\n",
    "    import math\n",
    "    import random\n",
    "    import argparse\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import requests\n",
    "\n",
    "    batch_size = 100\n",
    "    input_dim = 257 # every byte plus a special padding symbol\n",
    "    padding_char = 256\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--gpus', help='number of GPUs', default=1)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    ngpus = int(args.gpus)\n",
    "\n",
    "    if os.path.exists('malconv.h5'):\n",
    "        print(\"restoring malconv.h5 from disk for continuation training...\")\n",
    "        from keras.models import load_model\n",
    "        basemodel = load_model('malconv.h5')\n",
    "        _, maxlen, embedding_size = basemodel.layers[1].output_shape\n",
    "        input_dim\n",
    "    else:\n",
    "        maxlen = 2**20 # 1MB\n",
    "        embedding_size = 8 \n",
    "\n",
    "        # define model structure\n",
    "        inp = Input( shape=(maxlen,))\n",
    "        emb = Embedding( input_dim, embedding_size )( inp )\n",
    "        filt = Conv1D( filters=128, kernel_size=500, strides=500, use_bias=True, activation='relu', padding='valid' )(emb)\n",
    "        attn = Conv1D( filters=128, kernel_size=500, strides=500, use_bias=True, activation='sigmoid', padding='valid')(emb)\n",
    "        gated = Multiply()([filt,attn])\n",
    "        feat = GlobalMaxPooling1D()( gated )\n",
    "        dense = Dense(128, activation='relu')(feat)\n",
    "        outp = Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "        basemodel = Model( inp, outp )\n",
    "\n",
    "    basemodel.summary() \n",
    "\n",
    "    print(\"Using %i GPUs\" %ngpus)\n",
    "\n",
    "    if ngpus > 1:\n",
    "        model = multi_gpu.make_parallel(basemodel,ngpus)\n",
    "    else:\n",
    "        model = basemodel\n",
    "\n",
    "    from keras.optimizers import SGD\n",
    "    model.compile( loss='binary_crossentropy', optimizer=SGD(lr=0.01,momentum=0.9,nesterov=True,decay=1e-3), metrics=[metrics.binary_accuracy] )\n",
    "\n",
    "    def bytez_to_numpy(bytez,maxlen):\n",
    "        b = np.ones( (maxlen,), dtype=np.uint16 )*padding_char\n",
    "        bytez = np.frombuffer( bytez[:maxlen], dtype=np.uint8 )\n",
    "        b[:len(bytez)] = bytez\n",
    "        return b\n",
    "\n",
    "    def getfile_service(sha256,url=None,maxlen=maxlen):\n",
    "        if url is None:\n",
    "            raise NotImplementedError(\"You must provide your own url for getting file bytez by sha256\")\n",
    "        r = requests.get( url, params={'sha256':sha256} )\n",
    "        if not r.ok:\n",
    "            return None\n",
    "        return bytez_to_numpy( r.content, maxlen )        \n",
    "\n",
    "    def generator( hashes, labels, batch_size, shuffle=True ):\n",
    "        X = []\n",
    "        y = []\n",
    "        zipped = list(zip(hashes, labels))\n",
    "        while True:\n",
    "            if shuffle:\n",
    "                random.shuffle( zipped )\n",
    "            for sha256,l in zipped:\n",
    "                x = getfile_service(sha256)\n",
    "                if x is None:\n",
    "                    continue\n",
    "                X.append( x )\n",
    "                y.append( l )\n",
    "                if len(X) == batch_size:\n",
    "                    yield np.asarray(X,dtype=np.uint16), np.asarray(y)\n",
    "                    X = []\n",
    "                    y = []\n",
    "\n",
    "    import pandas as pd\n",
    "    train_labels = pd.read_csv('ember_training.csv.gz')\n",
    "    train_labels = train_labels[ train_labels['y'] != -1 ] # get only labeled samples\n",
    "    labels = train_labels['y'].tolist()\n",
    "    hashes = train_labels['sha256'].tolist()\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    hashes_train, hashes_val, labels_train, labels_val = train_test_split( hashes, labels, test_size=200 )\n",
    "\n",
    "    train_gen = generator( hashes_train, labels_train, batch_size )\n",
    "    val_gen = generator( hashes_val, labels_val, batch_size )\n",
    "\n",
    "    from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "    base = K.get_value( model.optimizer.lr )\n",
    "    def schedule(epoch):\n",
    "        return base / 10.0**(epoch//2)\n",
    "\n",
    "    model.fit_generator(\n",
    "        train_gen,\n",
    "        steps_per_epoch=len(hashes_train)//batch_size,\n",
    "        epochs=10,\n",
    "        validation_data=val_gen,\n",
    "        callbacks=[ LearningRateScheduler( schedule ) ],\n",
    "        validation_steps=int(math.ceil(len(hashes_val)/batch_size)),\n",
    "    )\n",
    "\n",
    "    basemodel.save('malconv.h5')\n",
    "\n",
    "    test_labels = pd.read_csv('ember_test.csv.gz')\n",
    "    labels_test = test_labels['y'].tolist()\n",
    "    hashes_test = test_labels['sha256'].tolist()\n",
    "\n",
    "    test_generator = generator(hashes_test,labels_test,batch_size=1,shuffle=False)\n",
    "    test_p = basemodel.predict_generator( test_generator, steps=len(test_labels), verbose=1 )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('*'*80)\n",
    "    print('''\n",
    "This is nonfunctional demonstration code that is provided for convenience. It shows\n",
    "- The MalConv structure used in our paper\n",
    "- Training procedure used in the paper\n",
    "- How to load the weights for the MalConv model that we used.\n",
    "\n",
    "It may be made functional by modifying the code to retrieve file contents by sha256\n",
    "from a user-defined URL.\n",
    "\n",
    "You may use the provided weights under the Ember AGPL-3.0 license included in the parent directory.\n",
    "We also ask that you cite the original MalConv paper and refer to the Ember paper as the implementation.\n",
    "\n",
    "(1) E. Raff, J. Barker, J. Sylvester, R. Brandon, B. Catanzaro, C. Nicholas, \"Malware Detection by Eating a Whole EXE\", in ArXiv e-prints. Oct. 2017.\n",
    "\n",
    "@ARTICLE{raff2017malware,\n",
    "  title={Malware detection by eating a whole exe},\n",
    "  author={Raff, Edward and Barker, Jon and Sylvester, Jared and Brandon, Robert and Catanzaro, Bryan and Nicholas, Charles},\n",
    "  journal={arXiv preprint arXiv:1710.09435},\n",
    "  year={2017}\n",
    "}\n",
    "\n",
    "(2) H. Anderson and P. Roth, \"EMBER: An Open Dataset for Training Static PE Malware Machine Learning Models”, in ArXiv e-prints. Apr. 2018.\n",
    "\n",
    "@ARTICLE{2018arXiv180404637A,\n",
    "  author = {{Anderson}, H.~S. and {Roth}, P.},\n",
    "  title = \"{EMBER: An Open Dataset for Training Static PE Malware Machine Learning Models}\",\n",
    "  journal = {ArXiv e-prints},\n",
    "  archivePrefix = \"arXiv\",\n",
    "  eprint = {1804.04637},\n",
    "  primaryClass = \"cs.CR\",\n",
    "  keywords = {Computer Science - Cryptography and Security},\n",
    "  year = 2018,\n",
    "  month = apr,\n",
    "  adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180404637A},\n",
    "}\n",
    "''')\n",
    "    print('*'*80)\n",
    "\n",
    "    #main() # uncomment this line after fixing the URL NotImplementedError above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
